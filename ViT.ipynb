{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184a1353",
   "metadata": {},
   "source": [
    " # このnotebookはViTの実装を行ったものです\n",
    " ## This notebook implement ViT\n",
    " \n",
    " ### 田村研究室 B4 西田裕一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ae7b1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "https://github.com/zisui-sukitarou/my-vit-pytorch/blob/main/vit_valid.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77e576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164768ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f0fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#証明書\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0672ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像のパッチ処理を行うクラス\n",
    "class VitInputLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels:int=3,\n",
    "        emb_dim:int=384,\n",
    "        num_patch_row:int=2,\n",
    "        image_size:int=32):\n",
    "        super(VitInputLayer,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_patch_row = num_patch_row\n",
    "        self.image_size = image_size\n",
    "        self.num_patch = self.num_patch_row**2\n",
    "        self.patch_size = int(self.image_size//self.num_patch_row)\n",
    "\n",
    "        self.patch_emb_layer=nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.emb_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "         # クラストークン \n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(1, 1, emb_dim) \n",
    "        )\n",
    "\n",
    "        # 位置埋め込み\n",
    "        ## クラストークンが先頭に結合されているため、\n",
    "        ## 長さemb_dimの位置埋め込みベクトルを(パッチ数+1)個用意 \n",
    "        self.pos_emb = nn.Parameter(\n",
    "            torch.randn(1, self.num_patch+1, emb_dim) \n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        引数:\n",
    "            x: 入力画像。形状は、(B, C, H, W)。[式(1)]\n",
    "                B: バッチサイズ、C:チャンネル数、H:高さ、W:幅\n",
    "        返り値:\n",
    "            z_0: ViTへの入力。形状は、(B, N, D)。\n",
    "                B:バッチサイズ、N:トークン数、D:埋め込みベクトルの長さ\n",
    "        \"\"\"\n",
    "        # パッチの埋め込み & flatten [式(3)]\n",
    "        ## パッチの埋め込み (B, C, H, W) -> (B, D, H/P, W/P) \n",
    "        ## ここで、Pはパッチ1辺の大きさ\n",
    "        z_0 = self.patch_emb_layer(x)\n",
    "\n",
    "        ## パッチのflatten (B, D, H/P, W/P) -> (B, D, Np) \n",
    "        ## ここで、Npはパッチの数(=H*W/Pˆ2)\n",
    "        z_0 = z_0.flatten(2)\n",
    "\n",
    "        ## 軸の入れ替え (B, D, Np) -> (B, Np, D) \n",
    "        z_0 = z_0.transpose(1, 2)\n",
    "\n",
    "        # パッチの埋め込みの先頭にクラストークンを結合 [式(4)] \n",
    "        ## (B, Np, D) -> (B, N, D)\n",
    "        ## N = (Np + 1)であることに留意\n",
    "        ## また、cls_tokenの形状は(1,1,D)であるため、\n",
    "        ## repeatメソッドによって(B,1,D)に変換してからパッチの埋め込みとの結合を行う \n",
    "        z_0 = torch.cat(\n",
    "            [self.cls_token.repeat(repeats=(x.size(0),1,1)), z_0], dim=1)\n",
    "\n",
    "        # 位置埋め込みの加算 [式(5)] \n",
    "        ## (B, N, D) -> (B, N, D) \n",
    "        z_0 = z_0 + self.pos_emb\n",
    "        return z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f4455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module): \n",
    "    def __init__(self, emb_dim:int=384, head:int=3, dropout:float=0.):\n",
    "        \"\"\" \n",
    "        引数:\n",
    "            emb_dim: 埋め込み後のベクトルの長さ \n",
    "            head: ヘッドの数\n",
    "            dropout: ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.head = head\n",
    "        self.emb_dim = emb_dim\n",
    "        self.head_dim = emb_dim // head\n",
    "        self.sqrt_dh = self.head_dim**0.5 # D_hの二乗根。qk^Tを割るための係数\n",
    "\n",
    "        # 入力をq,k,vに埋め込むための線形層。 [式(6)] \n",
    "        self.w_q = nn.Linear(emb_dim, emb_dim, bias=False) \n",
    "        self.w_k = nn.Linear(emb_dim, emb_dim, bias=False) \n",
    "        self.w_v = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "\n",
    "        # 式(7)にはないが、実装ではドロップアウト層も用いる \n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # MHSAの結果を出力に埋め込むための線形層。[式(10)]\n",
    "        ## 式(10)にはないが、実装ではドロップアウト層も用いる \n",
    "        self.w_o = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Dropout(dropout) \n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        引数:\n",
    "            z: MHSAへの入力。形状は、(B, N, D)。\n",
    "                B: バッチサイズ、N:トークンの数、D:ベクトルの長さ\n",
    "        返り値:\n",
    "            out: MHSAの出力。形状は、(B, N, D)。[式(10)]\n",
    "                B:バッチサイズ、N:トークンの数、D:埋め込みベクトルの長さ\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_patch, _ = z.size()\n",
    "\n",
    "        # 埋め込み [式(6)]\n",
    "        ## (B, N, D) -> (B, N, D)\n",
    "        q = self.w_q(z)\n",
    "        k = self.w_k(z)\n",
    "        v = self.w_v(z)\n",
    "\n",
    "        # q,k,vをヘッドに分ける [式(10)]\n",
    "        ## まずベクトルをヘッドの個数(h)に分ける\n",
    "        ## (B, N, D) -> (B, N, h, D//h)\n",
    "        q = q.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        k = k.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "        v = v.view(batch_size, num_patch, self.head, self.head_dim)\n",
    "\n",
    "        ## Self-Attentionができるように、\n",
    "        ## (バッチサイズ、ヘッド、トークン数、パッチのベクトル)の形に変更する \n",
    "        ## (B, N, h, D//h) -> (B, h, N, D//h)\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        # 内積 [式(7)]\n",
    "        ## (B, h, N, D//h) -> (B, h, D//h, N)\n",
    "        k_T = k.transpose(2, 3)\n",
    "        ## (B, h, N, D//h) x (B, h, D//h, N) -> (B, h, N, N) \n",
    "        dots = (q @ k_T) / self.sqrt_dh\n",
    "        ## 列方向にソフトマックス関数\n",
    "        attn = F.softmax(dots, dim=-1)\n",
    "        ## ドロップアウト\n",
    "        attn = self.attn_drop(attn)\n",
    "        # 加重和 [式(8)]\n",
    "        ## (B, h, N, N) x (B, h, N, D//h) -> (B, h, N, D//h) \n",
    "        out = attn @ v\n",
    "        ## (B, h, N, D//h) -> (B, N, h, D//h)\n",
    "        out = out.transpose(1, 2)\n",
    "        ## (B, N, h, D//h) -> (B, N, D)\n",
    "        out = out.reshape(batch_size, num_patch, self.emb_dim)\n",
    "\n",
    "        # 出力層 [式(10)]\n",
    "        ## (B, N, D) -> (B, N, D) \n",
    "        out = self.w_o(out) \n",
    "        return out\n",
    "\n",
    "#mhsa = MultiHeadSelfAttention()\n",
    "#out = mhsa(z_0) #z_0は2-2節のz_0=input_layer(x)で、形状は(B, N, D)\n",
    "\n",
    "# (2, 5, 384)(=(B, N, D))になっていることを確認 \n",
    "#print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3186a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 384])\n"
     ]
    }
   ],
   "source": [
    "class VitEncoderBlock(nn.Module): \n",
    "    def __init__(self, emb_dim:int=384, head:int=8, hidden_dim:int=384*4, dropout: float=0.):\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            emb_dim: 埋め込み後のベクトルの長さ\n",
    "            head: ヘッドの数\n",
    "            hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ \n",
    "                        原論文に従ってemb_dimの4倍をデフォルト値としている\n",
    "            dropout: ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(VitEncoderBlock, self).__init__()\n",
    "        # 1つ目のLayer Normalization [2-5-2項]\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        # MHSA [2-4-7項]\n",
    "        self.msa = MultiHeadSelfAttention(\n",
    "        emb_dim=emb_dim, head=head,\n",
    "        dropout = dropout,\n",
    "        )\n",
    "        # 2つ目のLayer Normalization [2-5-2項] \n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        # MLP [2-5-3項]\n",
    "        self.mlp = nn.Sequential( \n",
    "            nn.Linear(emb_dim, hidden_dim), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout), \n",
    "            nn.Linear(hidden_dim, emb_dim), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        引数:\n",
    "            z: Encoder Blockへの入力。形状は、(B, N, D)\n",
    "                B: バッチサイズ、N:トークンの数、D:ベクトルの長さ\n",
    "        返り値:\n",
    "            out: Encoder Blockへの出力。形状は、(B, N, D)。[式(10)]\n",
    "                B:バッチサイズ、N:トークンの数、D:埋め込みベクトルの長さ \n",
    "        \"\"\"\n",
    "        # Encoder Blockの前半部分 [式(12)] \n",
    "        out = self.msa(self.ln1(z)) + z\n",
    "        # Encoder Blockの後半部分 [式(13)] \n",
    "        out = self.mlp(self.ln2(out)) + out \n",
    "        return out\n",
    "\n",
    "#vit_enc = VitEncoderBlock()\n",
    "#z_1 = vit_enc(z_0) #z_0は2-2節のz_0=input_layer(x)で、形状は(B, N, D)\n",
    "\n",
    "# (2, 5, 384)(=(B, N, D))になっていることを確認 \n",
    "#print(z_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2b7b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "class Vit(nn.Module): \n",
    "    def __init__(self, in_channels:int=3, num_classes:int=10, emb_dim:int=384, num_patch_row:int=2, image_size:int=32, num_blocks:int=7, head:int=8, hidden_dim:int=384*4, dropout:float=0.):\n",
    "        \"\"\" \n",
    "        引数:\n",
    "            in_channels: 入力画像のチャンネル数\n",
    "            num_classes: 画像分類のクラス数\n",
    "            emb_dim: 埋め込み後のベクトルの長さ\n",
    "            num_patch_row: 1辺のパッチの数\n",
    "            image_size: 入力画像の1辺の大きさ。入力画像の高さと幅は同じであると仮定 \n",
    "            num_blocks: Encoder Blockの数\n",
    "            head: ヘッドの数\n",
    "            hidden_dim: Encoder BlockのMLPにおける中間層のベクトルの長さ \n",
    "            dropout: ドロップアウト率\n",
    "        \"\"\"\n",
    "        super(Vit, self).__init__()\n",
    "        # Input Layer [2-3節] \n",
    "        self.input_layer = VitInputLayer(\n",
    "            in_channels, \n",
    "            emb_dim, \n",
    "            num_patch_row, \n",
    "            image_size)\n",
    "\n",
    "        # Encoder。Encoder Blockの多段。[2-5節] \n",
    "        self.encoder = nn.Sequential(*[\n",
    "            VitEncoderBlock(\n",
    "                emb_dim=emb_dim,\n",
    "                head=head,\n",
    "                hidden_dim=hidden_dim,\n",
    "                dropout = dropout\n",
    "            )\n",
    "            for _ in range(num_blocks)])\n",
    "\n",
    "        # MLP Head [2-6-1項] \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            x: ViTへの入力画像。形状は、(B, C, H, W)\n",
    "                B: バッチサイズ、C:チャンネル数、H:高さ、W:幅\n",
    "        返り値:\n",
    "            out: ViTの出力。形状は、(B, M)。[式(10)]\n",
    "                B:バッチサイズ、M:クラス数 \n",
    "        \"\"\"\n",
    "        # Input Layer [式(14)]\n",
    "        ## (B, C, H, W) -> (B, N, D)\n",
    "        ## N: トークン数(=パッチの数+1), D: ベクトルの長さ \n",
    "        out = self.input_layer(x)\n",
    "        \n",
    "        # Encoder [式(15)、式(16)]\n",
    "        ## (B, N, D) -> (B, N, D)\n",
    "        out = self.encoder(out)\n",
    "\n",
    "        # クラストークンのみ抜き出す\n",
    "        ## (B, N, D) -> (B, D)\n",
    "        cls_token = out[:,0]\n",
    "\n",
    "        # MLP Head [式(17)]\n",
    "        ## (B, D) -> (B, M)\n",
    "        pred = self.mlp_head(cls_token)\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch_size, channel, height, width= 2, 3, 32, 32\n",
    "#x = torch.randn(batch_size, channel, height, width)\n",
    "#vit = Vit(in_channels=channel, num_classes=num_classes) \n",
    "#pred = vit(x)\n",
    "\n",
    "# (2, 10)(=(B, M))になっていることを確認 \n",
    "#print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = Vit(in_channels=channel, num_classes=num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vit.parameters(),lr=0.0001)\n",
    "#optimizer = optim.SGD(vit.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec85eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(0, epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    epoch_test_loss = 0\n",
    "    epoch_test_acc = 0\n",
    "\n",
    "    net.train()\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = vit(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()/len(train_loader)\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "        epoch_train_acc += acc/len(train_loader)\n",
    "\n",
    "        del inputs\n",
    "        del outputs\n",
    "        del loss\n",
    "\n",
    "    vit.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_test_loss += loss.item()/len(test_loader)\n",
    "            test_acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
    "            epoch_test_acc += test_acc/len(test_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1} : train acc. {epoch_train_acc:.2f} train loss {epoch_train_loss:.2f}')\n",
    "    print(f'Epoch {epoch+1} : test acc. {epoch_test_acc:.2f} test loss {epoch_test_loss:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
